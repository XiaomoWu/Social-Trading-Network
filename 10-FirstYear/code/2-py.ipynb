{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rossz\\OneDrive\\STN\\10-FirstYear\\code\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import aiohttp\n",
    "import pkuseg\n",
    "import pymongo\n",
    "import zhconv\n",
    "from types import TracebackType\n",
    "from typing import Dict, Optional, Type\n",
    "from aip import AipNlp\n",
    "from motor import motor_asyncio\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import DuplicateKeyError\n",
    "\n",
    "    \n",
    "# CONFIG\n",
    "if platform.system() == 'Windows':\n",
    "    # CODE_PATH\n",
    "    CODE_PATH = os.path.join(os.environ['onedrive'], 'STN/10-FirstYear/code')\n",
    "    DATA_PATH = os.path.join(os.environ['onedrive'], 'STN/10-FirstYear/data')\n",
    "    BERT_PATH = os.path.join(os.environ['onedrive'], 'Data/BERT')\n",
    "    ERNIE_PATH = os.path.join(os.environ['onedrive'], 'Data/baidu-paddle/LARK/ERNIE')\n",
    "    ERNIE_LAC_PATH=os.path.join(os.environ['onedrive'], 'Data/baidu-paddle/models/PaddleNLP/lexical_analysis')\n",
    "    %cd {CODE_PATH}\n",
    "elif platform.system() == 'Linux':\n",
    "    # CODE_PATH\n",
    "    CODE_PATH = '/mnt/c/Users/rossz/OneDrive/STN/10-FirstYear/code'\n",
    "    DATA_PATH = '/mnt/c/Users/rossz/OneDrive/STN/10-FirstYear/data'\n",
    "    %cd {CODE_PATH}\n",
    "\n",
    "# Baidu NLP\n",
    "def get_token(accounts):\n",
    "    '''get API tokens'''\n",
    "    url = f\"https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials\"\n",
    "    for account in accounts:\n",
    "        # client_id 为官网获取的AK， client_secret 为官网获取的SK\n",
    "        param = {'client_id': account['apiKey'],\n",
    "                 'client_secret': account['secretKey']}\n",
    "        resp = requests.get(url, param).json()\n",
    "        yield resp['access_token']\n",
    "        \n",
    "accounts = [{'appId': '16105374', # ross.zhu@outlook.com\n",
    "            'apiKey': 'zShobECEYX2B2CNrcz3QsM3I',\n",
    "            'secretKey': 'xM8g1lkhTd8b7DYGv2H9j7g2hvthPGE5'},\n",
    "           {'appId': '16218655', # 18868819096\n",
    "            'apiKey': 'AG2ygGZbCPqDW25ZC6FinQRy',\n",
    "            'secretKey': 'pcx8pFwjVhD0oGDsdm58bzt7Zm9zFpWk'},\n",
    "           {'appId': '16226200', # 13566633705\n",
    "            'apiKey': 'ymmGiPQoAoSF2KhcGVKNZEVj',\n",
    "            'secretKey': '78oQXsRtdmRcINEGtP3aP2y4sKA7HerU'},\n",
    "           {'appId': '16226243', # 13606586786\n",
    "            'apiKey': 'jIy3T25rYR1Kj9BlCKPcHbYe',\n",
    "            'secretKey': 'jORNEeu7Qnl6L1Hr07NomigmsUtXlloL'},\n",
    "           {'appId': '16230746', # 李锐\n",
    "            'apiKey': 't6sWsoIYinoGBgwbLiCopQHR',\n",
    "            'secretKey': 'evq4UygxQ555LXzVzaDWQS2TfGNUHrYf'},\n",
    "           {'appId': '16307961', # 俞嘉炜\n",
    "            'apiKey': 'TKIEhGe1fIY89VlYm8yjuDZW',\n",
    "            'secretKey': '6kDBxZnGZEsK4y9ntg94VOBSIjLS0UQs'},\n",
    "           {'appId': '16305932', # 芳菲\n",
    "            'apiKey': 'Y7NjBe2sqhfIjSIaB51wvlHH',\n",
    "            'secretKey': 'lqLW2XeSj0xBvCbUASh0E2wuMmW6e4ox'},\n",
    "           {'appId': '16305931', # 小毫生\n",
    "            'apiKey': 'D2GGEWS80KsU9mtft1WnuR0N',\n",
    "            'secretKey': '8FbvLeGD5aXLL6bpKSEZoG7CRt1Q9oqo'},\n",
    "           {'appId': '16306399', # 忻小宇\n",
    "            'apiKey': 'E89qU1OMrwvyanH7xixkFVrR',\n",
    "            'secretKey': 'ljPLgCtzOKLZuG56DPX9hvrVFHRsWHqf'},\n",
    "           {'appId': '16306475', # 杨老板\n",
    "            'apiKey': 'yehP04KXoWADTGUyLF8WK0hN',\n",
    "            'secretKey': 'jfp3wxqupbWfEoNi7L316LdfbUmDmRUo'},\n",
    "           ]\n",
    "tokens = list(get_token(accounts))\n",
    "baidu_clients = [AipNlp(**account) for account in accounts] \n",
    "\n",
    "# MongoDB-pymongo\n",
    "pymongo_client = MongoClient('localhost', 27018)\n",
    "db = pymongo_client.exchange\n",
    "coll_emb_bd = db.emb_bd\n",
    "coll_vocab_without_emb = db.vocab_without_emb\n",
    "coll_vocab_to_request = db.vocab_to_request\n",
    "coll_vocab_pos = db.vocab_pos\n",
    "coll_vocab_ne = db.vocab_ne\n",
    "coll_text_seg_bd = db.text_seg_bd\n",
    "coll_text_seg_pku = db.text_seg_pku\n",
    "coll_title_seg_bd = db.title_seg_bd\n",
    "coll_title_seg_pku = db.title_seg_pku\n",
    "\n",
    "coll_test = db.test\n",
    "\n",
    "# MongoDB-motor\n",
    "# loop=asyncio.new_event_loop()\n",
    "# asyncio.set_event_loop(loop)\n",
    "# motor_client = motor_asyncio.AsyncIOMotorClient('localhost', 27018, io_loop=loop)\n",
    "# db = motor_client.exchange\n",
    "# coll_mt_embedding_bd = db.embedding_bd\n",
    "# coll_mt_vocab_without_emb = db.vocab_without_emb\n",
    "# coll_mt_vocab_to_request = db.vocab_to_request\n",
    "# coll_mt_vocab_pos = db.vocab_pos\n",
    "# coll_mt_vocab_ne = db.vocab_ne\n",
    "# coll_mt_text_seg_bd = db.text_seg_bd\n",
    "# coll_mt_title_seg_bd = db.title_seg_bd\n",
    "# coll_mt_test = db.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create `pkl` from R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# cmt_text/title is created from R\n",
    "cmt = dt.fread('../data/cmt.csv', sep='\\t', encoding='utf-8')\n",
    "cmt_text = cmt[:, 'text'].to_list()[0]\n",
    "sv('cmt_text')\n",
    "cmt_title = cmt[:, 'title'].to_list()[0]\n",
    "sv('cmt_title')\n",
    "\n",
    "# every col except {'title', 'text'}\n",
    "cmt_without_title_and_text = cmt[:, {'id': f.id, 'user_id': f['user.id'],\n",
    "                     'created_at': f['created.at'],\n",
    "                     'source': f.source,\n",
    "                     'comment_id': f['comment.id'],\n",
    "                     'retweet_status_id': f['retweet.status.id'],\n",
    "                     'lastcrawl': f.lastcrawl}]\n",
    "cmt_without_title_and_text.to_csv('../data/cmt_no_text.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Clean `cmt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean cmt_text\n",
    "def clean_text(text: str):\n",
    "    import re\n",
    "    import zhconv\n",
    "    # text wrapped in <p>\n",
    "    p = re.compile(r'<p>(.*?)</p>')\n",
    "    text = p.sub(r' \\1 ', text)\n",
    "\n",
    "    # emoji\n",
    "    p = re.compile(r'<img.*?imedao.*?title=\"\\[(.+?)\\]\".*?alt=\"\\[(.+?)\\]\".*?/>')\n",
    "    text = p.sub(r' \\1 ', text)\n",
    "    \n",
    "    # @someone\n",
    "    # remove everything\n",
    "    p = re.compile(r'<a href[^#$]*?>(@.+?)</a>')\n",
    "    text = p.sub(r' ', text)\n",
    "    \n",
    "    # $stock$\n",
    "    p = re.compile(r'<a href[^@#]*?>\\$(.+?)[(（](\\w*?)(\\d*?)[)）]\\$</a>')\n",
    "    text = p.sub(r' \\1 \\2 \\3 ', text)\n",
    "    \n",
    "    # #摇一摇# xueqiu campain\n",
    "    p = re.compile(r'<a href[^@$]*?>(#.+?#)</a>')\n",
    "    text = p.sub(r' \\1 ', text)\n",
    "    \n",
    "    # user images\n",
    "    p = re.compile(r'<img.*?/>')\n",
    "    text = p.sub(r' ', text)\n",
    "    \n",
    "    # other hyperlinks\n",
    "    p = re.compile(r'<a href[^@$#]+?</a>')\n",
    "    text = p.sub(r' ', text)\n",
    "    \n",
    "    # other html tags\n",
    "    p = re.compile(r'(</?br? ?/?>|&nbsp;|</?p>|<h\\d>|</?strong>|&.+?dquo;|<p.+?>|&quot;)')\n",
    "    text = p.sub(r' ', text)\n",
    "    \n",
    "    # replace all duplicated space\n",
    "    p = re.compile(r'\\s+')\n",
    "    text = p.sub(r' ', text).strip()\n",
    "    \n",
    "    # all english letter to lower\n",
    "    text = text.lower()\n",
    "    # traditional to simplified\n",
    "    text = zhconv.convert(text, 'zh-cn')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cmt_title = ld('cmt_title')\n",
    "with Pool() as pool:\n",
    "    cmt_title_clean = list(tqdm(pool.imap(clean_text, cmt_title, chunksize=1024), total=len(cmt_title)))\n",
    "sv('cmt_title_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cmt_text = ld('cmt_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 44405159/44405159 [08:28<00:00, 87257.36it/s]\n"
     ]
    }
   ],
   "source": [
    "with Pool() as pool:\n",
    "    cmt_text_clean = list(tqdm(pool.imap(clean_text, cmt_text, chunksize=1024), total=len(cmt_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sv('cmt_text_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## PKU Seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "def seg_pku(text, coll_cmt):\n",
    "    segger = pkuseg.pkuseg(model_name='web')\n",
    "    print(f'scanning requested docs...{Now()}')\n",
    "    vocab_to_request = set(v['word'] for v in coll_vocab_to_request.find(projection={'word':1, '_id':0}))\n",
    "    vocab_with_emb = set(v['word'] for v in coll_emb_bd.find(projection={'word':1, '_id':0}))\n",
    "    vocab_without_emb = set(v['word'] for v in coll_vocab_without_emb.find(projection={'word':1, '_id':0}))\n",
    "    vocab = vocab_to_request | vocab_with_emb | vocab_without_emb\n",
    "    idx = set(doc['id'] for doc in coll_cmt.find(projection={'id':1, '_id':0}))\n",
    "    \n",
    "    # cut\n",
    "    print(f'start cutting...{Now()}')\n",
    "    for i, t in tqdm(enumerate(text), total=len(text)):\n",
    "        if t != '' and i not in idx :\n",
    "            segged = segger.cut(t)\n",
    "            try:\n",
    "                coll_cmt.insert_one({'id':i, 'text_seg':' '.join(segged)})\n",
    "                for word in segged:\n",
    "                    if word not in vocab:\n",
    "                        coll_vocab_to_request.insert_one({'word': word})\n",
    "            except DuplicateKeyError:\n",
    "                pass\n",
    "            except:\n",
    "                print(sys.exc_info()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vocab_to_request...10:17:52\n",
      "start cutting...10:18:06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 44405159/44405159 [13:53<00:00, 53289.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# title\n",
    "if 'cmt_title_clean' not in globals():\n",
    "    print(f'loading cmt_title_clean...{Now()}')\n",
    "    pickle.load(open('../data/cmt_title_clean.pkl', 'rb'))\n",
    "seg_pku(cmt_title_clean, coll_title_seg_pku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading cmt_text_clean...05:28:35\n",
      "scanning requested docs...05:29:58\n",
      "start cutting...05:32:31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 44405159/44405159 [00:14<00:00, 3052740.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# text\n",
    "if 'cmt_text_clean' not in globals():\n",
    "    print(f'loading cmt_text_clean...{Now()}')\n",
    "    cmt_text_clean = pickle.load(open('../data/cmt_text_clean.pkl', 'rb'))\n",
    "seg_pku(cmt_text_clean, coll_text_seg_pku)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Baidu API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def build_requests(cmt_path, cmt_coll):\n",
    "    cmt_text = pickle.load(open(cmt_path, 'rb'))\n",
    "    cmt_text = [(i, x) for i, x in enumerate(cmt_text)]\n",
    "\n",
    "    segged_cmt_id = set(x['id'] for x in cmt_coll.find(projection={'id':1, '_id':0}))\n",
    "    print(f'{len(segged_cmt_id)} docs have been segged.')\n",
    "    \n",
    "    request_cmt_id = set(range(len(cmt_text))) - segged_cmt_id\n",
    "    request_cmt = [(i, doc) for i, doc in cmt_text if i in request_cmt_id]\n",
    "    print(f'{len(request_cmt_id)} docs to reqeust.')\n",
    "    \n",
    "    vocab_to_request = set(x['word'] for x in coll_vocab_to_request.find(projection={'word':1, '_id':0}))\n",
    "    print(f'{len(vocab_to_request)} words to request')\n",
    "    \n",
    "    return request_cmt, vocab_to_request\n",
    "\n",
    "# request_cmt_title, request_cmt_id = build_requests('../data/cmt_title_clean.pkl', coll_title_seg_bd)\n",
    "request_cmt_text, vocab_to_request = build_requests('../data/cmt_text_clean.pkl', coll_text_seg_bd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_bd(corpus, coll):\n",
    "    # func to write coll_vocab\n",
    "    def write_vocab(coll, iterable, coll_type='to_request', pos=None, ne=None):\n",
    "        nonlocal count_duplicate, count_mongo_error\n",
    "        try:\n",
    "            with capture_output():\n",
    "                for word in iterable:\n",
    "                    if coll_type == 'to_request':\n",
    "                        coll.insert_one({'word': word})\n",
    "                    elif coll_type == 'pos':\n",
    "                        coll.insert_one({'word': word, coll_type: pos[word]})\n",
    "                    elif coll_type == 'ne':\n",
    "                        coll.insert_one({'word': word, coll_type: ne[word]})\n",
    "        except DuplicateKeyError:\n",
    "            pass\n",
    "        except:\n",
    "            count_mongo_error += 1\n",
    "            print(f'mongo error while inserting {coll.name}: {sys.exc_info()}')\n",
    "        \n",
    "    # build requests\n",
    "    # if request is empty, don't send!\n",
    "    id_last = set(x['id'] for x in coll.find(projection={'id':1, '_id':-1}))\n",
    "    print(f'text already segged: {len(id_last)}')\n",
    "    id_request = set([i for i, _ in corpus]) - id_last\n",
    "    corpus = [(i, doc) for i, doc in corpus if i in id_request and doc != '']\n",
    "    print(f'text to request: {len(corpus)}')\n",
    "    \n",
    "    # send request & process response\n",
    "    count_all, last_count_all, count_success, count_empty_input, count_input_too_long, count_limit_reached, count_other_error, count_duplicate, count_mongo_error = 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    ts = time.time()\n",
    "    for idx, text in corpus:\n",
    "        count_all += 1\n",
    "        time.sleep(0.02)\n",
    "        try:\n",
    "            # if empty request, directly write to mongo\n",
    "            request_text = text.encode('gbk', errors='ignore').decode('gbk')\n",
    "            if request_text == '': # after re-encoding, request may be empty\n",
    "                count_empty_input += 1\n",
    "                coll.insert_one({'id': idx, 'text_seg': '', 'text_seg2': ''})\n",
    "                count_success += 1\n",
    "            # else, send request and get response\n",
    "            else:\n",
    "                response = client_bd.lexer(request_text)\n",
    "                # process errors in response\n",
    "                if 'error_code' in response:\n",
    "                    if response['error_code'] == 18: # limit reached\n",
    "                        count_limit_reached += 1\n",
    "                    elif response['error_code'] == 282131: # input too long\n",
    "                        count_input_too_long += 1\n",
    "                    else:\n",
    "                        print(f'other error code: request:\"{request_text}\", response:\"{response}\"')\n",
    "                        count_other_error += 1\n",
    "                else:\n",
    "                    # process successful response\n",
    "                    response = response['items']\n",
    "                    vocab_coarse = [item['item'] for item in response]\n",
    "                    vocab_granular = list(itertools.chain.from_iterable([item['basic_words'] for item in response]))\n",
    "                    text_seg = ' '.join(vocab_coarse)\n",
    "                    text_seg2 = ' '.join(vocab_granular)\n",
    "                    pos = {item['item']: item['pos'] for item in response}\n",
    "                    ne = {item['item']: item['ne'] for item in response}\n",
    "\n",
    "                    # write coll_text_seg_bd\n",
    "                    with capture_output():\n",
    "                        coll.insert_one({'id': idx, 'text_seg': text_seg, 'text_seg2': text_seg2})\n",
    "                        count_success += 1\n",
    "\n",
    "                    # write coll_vocab_to_reqeust\n",
    "                    write_vocab(coll_vocab_to_request, vocab_coarse)\n",
    "                    write_vocab(coll_vocab_to_request, vocab_granular)\n",
    "                    # write coll_vocab_pos\n",
    "                    write_vocab(coll_vocab_pos, vocab_coarse, 'pos', pos=pos)\n",
    "                    # write coll_vocab_ne\n",
    "                    write_vocab(coll_vocab_ne, vocab_coarse, 'ne', ne=ne)\n",
    "            \n",
    "        except DuplicateKeyError:\n",
    "            count_duplicate += 1\n",
    "        except:\n",
    "            print(f'unknow error: {sys.exc_info()}')\n",
    "            count_other_error += 1\n",
    "        finally:\n",
    "            # log\n",
    "            if count_all % 200 == 0:\n",
    "                print(f'{count_all}/{len(corpus)}, {Now()} ({time.time()-ts:.1f}s / {(count_all-last_count_all)/(time.time()-ts):.1f}QPS)')\n",
    "                print(f'success:{count_success}, limit-reached:{count_limit_reached}, empty-input-in-success:{count_empty_input}, dup:{count_duplicate}, input-too-long:{count_input_too_long}, other-baidu-error:{count_other_error}, other-mongo-error:{count_mongo_error}')\n",
    "                print('---------------------')\n",
    "                count_success, count_empty_input, count_input_too_long, count_limit_reached, count_other_error, count_duplicate, count_mongo_error = 0, 0, 0, 0, 0, 0, 0\n",
    "                ts = time.time()\n",
    "                last_count_all = count_all\n",
    "                      \n",
    "# text/title\n",
    "seg_bd(cmt_title_clean[40000000:], coll_title_seg_bd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_lex(text, token):\n",
    "    base_url = 'https://aip.baidubce.com/rpc/2.0/nlp/v1/lexer'\n",
    "    url_params = {'access_token': token,\n",
    "                  'charset': 'UTF-8'}\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    request_text = {'text': text}\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.request('POST', url=base_url, json=request_text, headers=headers, params=url_params) as resp:\n",
    "            response = await resp.json()\n",
    "            return response\n",
    "        \n",
    "async def seg_one(idx, text, token, coll_cmt, bucket, download_delay, len_tasks):\n",
    "    global t0, count_all, last_count_all, count_success, count_empty_input,\\\n",
    "        count_input_too_long, count_limit_reached,\\\n",
    "        count_other_error, count_duplicate, count_mongo_error,\\\n",
    "        vocab_to_request\n",
    "    try:\n",
    "        \n",
    "        # if empty text, directly write to db\n",
    "        # else, send request and get response\n",
    "        if text == '':\n",
    "            count_empty_input += 1\n",
    "            coll_cmt.insert_one({'id': idx, 'text_seg': '', 'text_seg2': ''})\n",
    "        else:\n",
    "            await asyncio.sleep(download_delay)\n",
    "            # if longer than 20000 bytes, truncate\n",
    "            is_too_long = False\n",
    "            if len(text.encode('utf-8')) >= 20000:\n",
    "                text = text.encode('utf-8')[:20000].decode('utf-8', errors='ignore')\n",
    "                is_too_long = True\n",
    "            async with bucket:\n",
    "                    response = await get_lex(text, token)\n",
    "\n",
    "        # process errors in response\n",
    "        # if no error, generate input doc\n",
    "        if 'error_code' in response:\n",
    "            if response['error_code'] == 18: # limit reached\n",
    "                count_limit_reached += 1\n",
    "            elif response['error_code'] == 282131: # input too long\n",
    "                count_input_too_long += 1\n",
    "            else:\n",
    "                print(f'other baidu error: request:\"{text[:20]}...\", response:\"{response}\"')\n",
    "                print(token)\n",
    "                count_other_error += 1\n",
    "        else:\n",
    "            # process successful response\n",
    "            response = response['items']\n",
    "            vocab_coarse = [item['item'] for item in response]\n",
    "            vocab_granular = list(itertools.chain.from_iterable([item['basic_words'] for item in response]))\n",
    "            text_seg = ' '.join(vocab_coarse)\n",
    "            text_seg2 = ' '.join(vocab_granular)\n",
    "            pos = {item['item']: item['pos'] for item in response if item['pos'] != ''}\n",
    "            ne = {item['item']: item['ne'] for item in response if item['ne'] != ''}\n",
    "\n",
    "            # write coll_text_seg_bd; coll_vocab_to_request\n",
    "            with capture_output():\n",
    "                coll_cmt.insert_one({'id': idx, 'text_seg': text_seg, 'text_seg2': text_seg2, 'is_too_long': is_too_long})\n",
    "                count_success += 1\n",
    "                for word in itertools.chain(vocab_coarse, vocab_granular):\n",
    "                    if word not in vocab_to_request:\n",
    "                        coll_vocab_to_request.insert_one({'word': word})\n",
    "                \n",
    "    except DuplicateKeyError:\n",
    "        count_duplicate += 1\n",
    "    except:\n",
    "        print(f'Unknow Exception: {sys.exc_info()[:-1]}')\n",
    "        count_other_error += 1\n",
    "    finally:\n",
    "    # log\n",
    "        if count_all % 10000 == 0:\n",
    "            print(f'{count_all}/{len_tasks}, {Now()} ({time.time()-t0:.1f}s / {(count_all-last_count_all)/(time.time()-t0):.1f}QPS)')\n",
    "            print(f'success:{count_success}, limit-reached:{count_limit_reached}, empty-input-in-success:{count_empty_input}, dup:{count_duplicate}, input-too-long:{count_input_too_long}, other-baidu-error:{count_other_error}, other-mongo-error:{count_mongo_error}')\n",
    "            print('---------------------')\n",
    "            count_success, count_empty_input, count_input_too_long, count_limit_reached, count_other_error, count_duplicate, count_mongo_error = 0, 0, 0, 0, 0, 0, 0\n",
    "            t0 = time.time()\n",
    "            last_count_all = count_all\n",
    "    \n",
    "    \n",
    "async def download_many(tokens, request_cmt, coll_cmt, max_concur_req, download_delay):\n",
    "    global count_all\n",
    "    tokens_iter = itertools.cycle(tokens)\n",
    "    len_tasks = len(request_cmt)\n",
    "    # restrict concurrenty\n",
    "    bucket = AsyncLeakyBucket(max_concur_req, 1)\n",
    "    \n",
    "    # generate tasks\n",
    "    print(f'generating tasks...{Now()}')\n",
    "    tasks = []\n",
    "    for idx, text in request_cmt:\n",
    "        task = seg_one(idx, text, next(tokens_iter), coll_cmt, bucket, download_delay, len_tasks)\n",
    "        tasks.append(task)\n",
    "    print(f'tasks generated {Now()}')\n",
    "        \n",
    "    # schedule tasks\n",
    "    for future in tqdm(asyncio.as_completed(tasks), total=len_tasks):\n",
    "        count_all += 1\n",
    "        await future\n",
    "\n",
    "# run the main\n",
    "t0 = time.time()\n",
    "count_all, last_count_all, count_success, count_empty_input, count_input_too_long, count_limit_reached, count_other_error, count_duplicate, count_mongo_error = 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "\n",
    "asyncio.run(download_many(tokens, request_cmt_text, coll_text_seg_bd, max_concur_req=31, download_delay=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ERNIE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ERNIE to segment is too memory expensive, give up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# infer_set = {ERNIE_LAC_PATH}/data/pred.tsv\n",
    "infer_set = f'{DATA_PATH}/cmt_test_1x.tsv'\n",
    "\n",
    "%cd {ERNIE_LAC_PATH}\n",
    "!python \"{ERNIE_LAC_PATH}/run_ernie_sequence_labeling.py\" --ernie_config_path \"{ERNIE_PATH}/pretrained/ernie_config.json\" --init_pretraining_params \"{ERNIE_PATH}/pretrained/params/\" --init_checkpoint \"{ERNIE_LAC_PATH}/model_finetuned\" --init_bound 0.1 --vocab_path \"{ERNIE_PATH}/pretrained/vocab.txt\" --batch_size 8 --random_seed 0 --num_labels 57 --max_seq_len 64 --infer_set {infer_set} --label_map_config \"{ERNIE_LAC_PATH}/conf/label_map.json\" --do_lower_case true --use_cuda true --do_train false --do_test false --do_infer true\n",
    "# !cd {CODE_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create reqeust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# vocab_last\n",
    "vocab_with_emb = set(doc['word'] for doc in coll_emb_bd.find(projection={'word':1, '_id':0}))\n",
    "print(f'vocab_with_emb: {len(vocab_with_emb)}')\n",
    "   \n",
    "# vocab_without_emb\n",
    "vocab_without_emb = set(doc['word'] for doc in coll_vocab_without_emb.find(projection={'word':1, '_id':0}))\n",
    "print(f'vocab_without_emb: {len(vocab_without_emb)}')\n",
    "\n",
    "# delete dup in coll_vocab_to_request\n",
    "vocab_to_request = set(doc['word'] for doc in coll_vocab_to_request.find(projection={'word':1, '_id':0}))\n",
    "vocab_dup =  (vocab_to_request & vocab_with_emb) | (vocab_to_request & vocab_without_emb)\n",
    "\n",
    "for word in vocab_dup:\n",
    "    with capture_output():\n",
    "        coll_vocab_to_request.delete_one({'word':word})\n",
    "\n",
    "# vocab_request\n",
    "vocab_request = vocab_to_request - vocab_with_emb - vocab_without_emb\n",
    "\n",
    "# # vocab from GBK\n",
    "# gbk_path = os.path.expanduser('~')+'/OneDrive/Data/gbk-list.txt'\n",
    "# gbk_list = []\n",
    "# with open(gbk_path, encoding='utf-8') as f:\n",
    "#     for line in f:\n",
    "#         line = line.replace('\\n', '').strip()\n",
    "#         gbk_list.append(line)\n",
    "# vocab_request = set(gbk_list) - vocab_without_emb - vocab_last\n",
    "\n",
    "print(f'vocab_request: {len(vocab_request)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## send requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_all, last_count_all, count_success, count_non_exist, count_input_too_long, count_input_empty,\\\n",
    "count_limit_reached, count_unknow_error, count_mongo_error, count_duplicate = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "t0 = time.time()\n",
    "\n",
    "for word in tqdm(vocab_request):\n",
    "    count_all += 1\n",
    "#     time.sleep(0.1)\n",
    "    try:\n",
    "        response = client_bd.wordEmbedding(word)\n",
    "        if 'error_code' in response:\n",
    "            if response['error_code'] == 18: # limit reached\n",
    "                count_limit_reached += 1\n",
    "            elif response['error_code'] == 282131: # input too long / input empty\n",
    "                count_input_too_long += 1\n",
    "            elif response['error_code'] == 282134:\n",
    "                count_input_empty += 1\n",
    "            elif response['error_code'] == 282300: # non-existing\n",
    "                count_non_exist += 1\n",
    "                with capture_output() as captured:\n",
    "                    coll_vocab_non_exist.insert_one({'word': word})\n",
    "            else:\n",
    "                print(f'Other error_code: word:\"{word}\", response:\"{response}\"')\n",
    "                count_unknow_error += 1\n",
    "        elif 'vec' in response:\n",
    "            vec = response['vec']\n",
    "            with capture_output() as captured:\n",
    "                assert type(vec) is list, 'type(vec) is not list: {word}'\n",
    "                assert len(vec) == 1024, 'len(vec) should be 1024'\n",
    "                coll_embedding_bd.insert_one({'word': word, 'vec': vec})\n",
    "                count_success += 1\n",
    "    except DuplicateKeyError:\n",
    "        count_duplicate += 1\n",
    "    except:\n",
    "        print(f\"Exception: {sys.exc_info()[:-1]}, {Now()}\")\n",
    "        count_unknow_error += 1\n",
    "    finally:\n",
    "        if count_all % 200 == 0:\n",
    "            te = time.time()\n",
    "            print(f'{count_all}/{len(vocab_request)}, {Now()} ({te-t0:.1f}s / {(count_all-last_count_all)/(te-t0):.1f}QPS)')\n",
    "            print(f'success:{count_success} non_existing:{count_non_exist} duplicate:{count_duplicate} limit_reached:{count_limit_reached} input_too_long:{count_input_too_long} input_empty:{count_input_empty} unkown_error:{count_unknow_error}')\n",
    "            print('---------------------')\n",
    "            t0 = time.time()\n",
    "            count_success, count_non_exist, count_duplicate, count_input_too_long, count_input_empty,\\\n",
    "            count_limit_reached, count_unknow_error, count_mongo_error = 0, 0, 0, 0, 0, 0, 0, 0\n",
    "            last_count_all = count_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_emb(word, token):\n",
    "    base_url = 'https://aip.baidubce.com/rpc/2.0/nlp/v2/word_emb_vec'\n",
    "    url_params = {'access_token': token,\n",
    "                  'charset': 'UTF-8'}\n",
    "    request_text = {'word': word}\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.request('POST', url=base_url, json=request_text, headers=headers, params=url_params) as resp:\n",
    "            response = await resp.json()\n",
    "            return response\n",
    "\n",
    "async def request_one(word, token, bucket, download_delay):\n",
    "    global count_all, last_count_all, count_success,\\\n",
    "        count_non_exist, count_input_too_long, count_input_empty,\\\n",
    "        count_limit_reached, count_unknow_error,\\\n",
    "        count_mongo_error, count_duplicate, t0\n",
    "    try:\n",
    "        await asyncio.sleep(download_delay)\n",
    "        async with bucket:\n",
    "                response = await get_emb(word, token)\n",
    "        if 'error_code' in response:\n",
    "            if response['error_code'] == 18: # limit reached\n",
    "                count_limit_reached += 1\n",
    "            elif response['error_code'] == 282131: # input too long\n",
    "                count_input_too_long += 1\n",
    "            elif response['error_code'] == 282134: # input empty\n",
    "                count_input_empty += 1\n",
    "            elif response['error_code'] == 282300: # non-existing\n",
    "                count_non_exist += 1\n",
    "                with capture_output() as captured:\n",
    "                    coll_vocab_without_emb.insert_one({'word': word})\n",
    "            else:\n",
    "                print(f'Other error_code: word:\"{word}\", response:\"{response}\"')\n",
    "                count_unknow_error += 1\n",
    "        elif 'vec' in response:\n",
    "            vec = response['vec']\n",
    "            with capture_output() as captured:\n",
    "                assert type(vec) is list, 'type(vec) is not list: {word}'\n",
    "                assert len(vec) == 1024, 'len(vec) should be 1024'\n",
    "                coll_emb_bd.insert_one({'word': word, 'vec': vec})\n",
    "                count_success += 1\n",
    "    except DuplicateKeyError:\n",
    "        count_duplicate += 1\n",
    "    except:\n",
    "        print(f\"Unknow exception: {sys.exc_info()[:-1]}, {Now()}\")\n",
    "        count_unknow_error += 1\n",
    "    finally:\n",
    "        if count_all % 10000 == 0:\n",
    "            te = time.time()\n",
    "            print(f'{count_all}/{len(vocab_request)}, {Now()} ({te-t0:.1f}s / {(count_all-last_count_all)/(te-t0):.1f}QPS)')\n",
    "            print(f'success:{count_success} non_existing:{count_non_exist} duplicate:{count_duplicate} limit_reached:{count_limit_reached} input_too_long:{count_input_too_long} input_empty:{count_input_empty} unkown_error:{count_unknow_error}')\n",
    "            print('---------------------')\n",
    "            t0 = time.time()\n",
    "            count_success, count_non_exist, count_duplicate, count_input_too_long, count_input_empty,\\\n",
    "            count_limit_reached, count_unknow_error, count_mongo_error = 0, 0, 0, 0, 0, 0, 0, 0\n",
    "            last_count_all = count_all\n",
    "    \n",
    "async def download_many(tokens, vocab_request, max_concur_req, download_delay):\n",
    "    global count_all\n",
    "    tokens_iter = itertools.cycle(tokens)\n",
    "    # restrict concurrenty\n",
    "    bucket = AsyncLeakyBucket(max_concur_req, 1)\n",
    "    # generate tasks\n",
    "    print(f'start creating tasks... {Now()}')\n",
    "    tasks = []\n",
    "    for word in vocab_request:\n",
    "        task = request_one(word, next(tokens_iter), bucket, download_delay)\n",
    "        tasks.append(task)\n",
    "    print(f'tasks succesfully created {Now()}')\n",
    "        \n",
    "    # schedule tasks\n",
    "    for future in tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n",
    "        count_all += 1\n",
    "        await future\n",
    "\n",
    "# run\n",
    "t0 = time.time()\n",
    "\n",
    "count_all, last_count_all, count_success, count_non_exist, count_input_too_long, count_input_empty, count_limit_reached, count_unknow_error, count_mongo_error, count_duplicate = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "\n",
    "asyncio.run(download_many(tokens, vocab_request, max_concur_req=32, download_delay=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Doc Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## ERNIE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### create `tsv`\n",
    "\n",
    "- for *text*:\n",
    "    - 44,405,159 original docs\n",
    "    \n",
    "        - 80.9% are shorter than 64\n",
    "\n",
    "        - 89.5% are shorter than 128\n",
    "\n",
    "        - 96.8% are shorter than 513\n",
    "\n",
    "        - 99.2% are shorter than 2048\n",
    "        \n",
    "    - if split into subdocs of 128 char \n",
    "        - 53,173,056 sub-docs (max_doc_size=512)\n",
    "    \n",
    "        - 57,705,937 sub-docs (max_doc_size=1024)\n",
    "    \n",
    "        - **62,220,599 sub-docs (max_doc_size=2048)**\n",
    "    \n",
    "    - if split into sentences\n",
    "        - 155,914,426 sentences\n",
    "        \n",
    "            - 64.1% shorter than 32\n",
    "\n",
    "            - 89.9% shorter than 64\n",
    "            \n",
    "        - 178,013,470 sub-sents (64 char)\n",
    "        \n",
    "        - 249,078,800 sub-sents (32 char)\n",
    "    \n",
    "- for *title*:\n",
    "\n",
    "    - 1,173,406 non-empty docs\n",
    "        - 96.9% are shorter than 32 char\n",
    "        \n",
    "    - **1,199,615 sub-docs**\n",
    "        - split level: 32 char\n",
    "        \n",
    "    - 1,433,301 sentences\n",
    "        - split size: sentence\n",
    "        \n",
    "        - 98.45% are shorter than 32 char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subdoc(cmt, cmt_type, subdoc_size, max_doc_size=None, file_size=None):\n",
    "    # split every doc into chunks of size 64\n",
    "    # also preserve its original row_id\n",
    "    # empty docs not included\n",
    "    cmt_subdoc_id = []\n",
    "    cmt_subdoc = []\n",
    "    for idx, doc in enumerate(cmt):\n",
    "        if doc != '':\n",
    "            if max_doc_size is not None:\n",
    "                doc = doc[:max_doc_size]\n",
    "            subdoc = list(chunks(doc, subdoc_size))\n",
    "            idx = [idx] * len(subdoc)\n",
    "            cmt_subdoc.extend(subdoc)\n",
    "            cmt_subdoc_id.extend(idx)\n",
    "    print(f'{len(cmt_subdoc)} non-empty sub-docs')\n",
    "            \n",
    "    for idx, file in enumerate(chunks(cmt_subdoc, file_size)):\n",
    "        with open(f'../data/cmt_{cmt_type}_subdoc_{idx}.tsv', 'w', encoding='utf-8') as f:\n",
    "            with capture_output():\n",
    "                f.write('text_a\\tlabel\\n')\n",
    "                for doc in file:\n",
    "                    f.write(f'{doc}\\t\\n')\n",
    "                    \n",
    "    pickle.dump(cmt_subdoc_id, open(f'../data/cmt_{cmt_type}_subdoc_id.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 245 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create test file for speed benchmarking\n",
    "factor = 8\n",
    "with open(f'../data/cmt_test_{factor}x.tsv', 'w', encoding='utf-8') as f:\n",
    "    if 'cmt_text_clean' not in globals():\n",
    "        cmt_text_clean = ld('cmt_text_clean')\n",
    "    with capture_output():\n",
    "        f.write('text_a\\tlabel\\n')\n",
    "        for doc in cmt_text_clean[:20000]*factor:\n",
    "            f.write(f'{doc}\\t\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create title tsv\n",
    "if 'cmt_title_clean' not in globals():\n",
    "    cmt_title_clean = ld('cmt_title_clean')\n",
    "create_subdoc(cmt_title_clean, cmt_type='title', doc_size=32, file_size=2000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62220599 non-empty sub-docs\n",
      "Wall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create text tsv\n",
    "if 'cmt_text_clean' not in globals():\n",
    "    cmt_text_clean = ld('cmt_text_clean')\n",
    "create_subdoc(cmt_text_clean, cmt_type='text', subdoc_size=128, max_doc_size=2048, file_size=2000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_emb(cmt_type, emb_level: 'subdoc or sentence', max_seq_len, batch_size, finished_file_idx):\n",
    "    all_file_idx = set(int(re.search(r'\\d+', str(x)).group()) for x in Path('../data').glob(f'cmt_{cmt_type}_{emb_level}_*.tsv'))\n",
    "    unfinished_file_idx = all_file_idx - finished_file_idx\n",
    "    \n",
    "    print(f'{len(unfinished_file_idx)} file(s) to process...')\n",
    "    for i, file_idx in enumerate(unfinished_file_idx):\n",
    "        file_path = f'../data/cmt_{cmt_type}_{emb_level}_{file_idx}.tsv'\n",
    "\n",
    "        print(f'{i+1}/{len(unfinished_file_idx)} (file_idx:{file_idx}) started at {Now()}')\n",
    "        t0 = time.time()\n",
    "        with capture_output():\n",
    "            !python {ERNIE_PATH}/ernie_encoder.py --cmt_type {cmt_type} --file_idx {file_idx} --use_cuda true --batch_size {batch_size} --output_dir {DATA_PATH} --init_pretraining_params {ERNIE_PATH}/pretrained/params --data_set {file_path} --vocab_path {ERNIE_PATH}/config/vocab.txt --max_seq_len {max_seq_len} --ernie_config_path {ERNIE_PATH}/config/ernie_config.json\n",
    "        print(f'{i+1}/{len(unfinished_file_idx)} finshed, use {time.time()-t0:.1f}s')\n",
    "        print('-------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 file(s) to process...\n",
      "1/1 started at 15:50:06\n",
      "1/1 finshed, use 1199.6s\n",
      "-------------------------\n",
      "Wall time: 19min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get title\n",
    "get_doc_emb(cmt_type='title',\n",
    "            emb_level='subdoc',\n",
    "            max_seq_len=32,\n",
    "            batch_size=768,\n",
    "            finished_file_idx={0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 file(s) to process...\n",
      "1/32 started at 16:46:50\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get text\n",
    "get_doc_emb(cmt_type='text',\n",
    "            emb_level='subdoc',\n",
    "            max_seq_len=128,\n",
    "            batch_size=128,\n",
    "            finished_file_idx={0, 1, 10, 11})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 37.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# sample\n",
    "cmt_text_clean = ld('cmt_text_clean2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# wreite cmt_text_clean\n",
    "chunk_size = 1000000\n",
    "n_chunk = len(cmt_text_clean)//chunk_size+1\n",
    "\n",
    "for i in range(n_chunk):\n",
    "    start = chunk_size * i\n",
    "    end = start + chunk_size\n",
    "    with open(f'../data/cmt_text_clean_{i}.txt', 'w', encoding='utf-8') as f:\n",
    "        with capture_output():\n",
    "#             f.write('text_a\\tlabel\\n')\n",
    "            for doc in cmt_text_clean[start:end]:\n",
    "                f.write(f'{doc}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(Now())\n",
    "    input_file = f'cmt_text_clean_{i}'\n",
    "    output_file = f'cmt_text_emb_{i}'\n",
    "    \n",
    "    !python {BERT_PATH}/extract_features.py --input_file={DATA_PATH}/{input_file}.txt --output_file={DATA_PATH}/{output_file}.pkl --vocab_file={BERT_PATH}/chinese_L-12_H-768_A-12/vocab.txt --bert_config_file={BERT_PATH}/chinese_L-12_H-768_A-12/bert_config.json --init_checkpoint={BERT_PATH}/chinese_L-12_H-768_A-12/bert_model.ckpt --layers=-1,-2,-3,-4 --max_seq_length=512 --batch_size=16\n",
    "    print(Now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ld('output')\n",
    "len(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc-autonumbering": true,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

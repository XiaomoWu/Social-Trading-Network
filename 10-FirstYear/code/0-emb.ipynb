{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rossz\\OneDrive\\STN\\10-FirstYear\\code\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import aiohttp\n",
    "import pkuseg\n",
    "import pymongo\n",
    "import zhconv\n",
    "from types import TracebackType\n",
    "from typing import Dict, Optional, Type\n",
    "from aip import AipNlp\n",
    "from motor import motor_asyncio\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import DuplicateKeyError\n",
    "\n",
    "    \n",
    "# CONFIG\n",
    "if platform.system() == 'Windows':\n",
    "    # CODE_PATH\n",
    "    CODE_PATH = os.path.join(os.environ['onedrive'], 'STN/10-FirstYear/code')\n",
    "    DATA_PATH = os.path.join(os.environ['onedrive'], 'STN/10-FirstYear/data')\n",
    "    BERT_PATH = os.path.join(os.environ['onedrive'], 'Data/BERT')\n",
    "    ERNIE_PATH = os.path.join(os.environ['onedrive'], 'Data/baidu-paddle/LARK/ERNIE')\n",
    "    ERNIE_LAC_PATH=os.path.join(os.environ['onedrive'], 'Data/baidu-paddle/models/PaddleNLP/lexical_analysis')\n",
    "    %cd {CODE_PATH}\n",
    "elif platform.system() == 'Linux':\n",
    "    # CODE_PATH\n",
    "    CODE_PATH = '/mnt/c/Users/rossz/OneDrive/STN/10-FirstYear/code'\n",
    "    DATA_PATH = '/mnt/c/Users/rossz/OneDrive/STN/10-FirstYear/data'\n",
    "    %cd {CODE_PATH}\n",
    "\n",
    "# Baidu NLP\n",
    "def get_token(accounts):\n",
    "    '''get API tokens'''\n",
    "    url = f\"https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials\"\n",
    "    for account in accounts:\n",
    "        # client_id 为官网获取的AK， client_secret 为官网获取的SK\n",
    "        param = {'client_id': account['apiKey'],\n",
    "                 'client_secret': account['secretKey']}\n",
    "        resp = requests.get(url, param).json()\n",
    "        yield resp['access_token']\n",
    "        \n",
    "accounts = [{'appId': '16105374', # ross.zhu@outlook.com\n",
    "            'apiKey': 'zShobECEYX2B2CNrcz3QsM3I',\n",
    "            'secretKey': 'xM8g1lkhTd8b7DYGv2H9j7g2hvthPGE5'},\n",
    "           {'appId': '16218655', # 18868819096\n",
    "            'apiKey': 'AG2ygGZbCPqDW25ZC6FinQRy',\n",
    "            'secretKey': 'pcx8pFwjVhD0oGDsdm58bzt7Zm9zFpWk'},\n",
    "           {'appId': '16226200', # 13566633705\n",
    "            'apiKey': 'ymmGiPQoAoSF2KhcGVKNZEVj',\n",
    "            'secretKey': '78oQXsRtdmRcINEGtP3aP2y4sKA7HerU'},\n",
    "           {'appId': '16226243', # 13606586786\n",
    "            'apiKey': 'jIy3T25rYR1Kj9BlCKPcHbYe',\n",
    "            'secretKey': 'jORNEeu7Qnl6L1Hr07NomigmsUtXlloL'},\n",
    "           {'appId': '16230746', # 李锐\n",
    "            'apiKey': 't6sWsoIYinoGBgwbLiCopQHR',\n",
    "            'secretKey': 'evq4UygxQ555LXzVzaDWQS2TfGNUHrYf'},\n",
    "           {'appId': '16307961', # 俞嘉炜\n",
    "            'apiKey': 'TKIEhGe1fIY89VlYm8yjuDZW',\n",
    "            'secretKey': '6kDBxZnGZEsK4y9ntg94VOBSIjLS0UQs'},\n",
    "           {'appId': '16305932', # 芳菲\n",
    "            'apiKey': 'Y7NjBe2sqhfIjSIaB51wvlHH',\n",
    "            'secretKey': 'lqLW2XeSj0xBvCbUASh0E2wuMmW6e4ox'},\n",
    "           {'appId': '16305931', # 小毫生\n",
    "            'apiKey': 'D2GGEWS80KsU9mtft1WnuR0N',\n",
    "            'secretKey': '8FbvLeGD5aXLL6bpKSEZoG7CRt1Q9oqo'},\n",
    "           {'appId': '16306399', # 忻小宇\n",
    "            'apiKey': 'E89qU1OMrwvyanH7xixkFVrR',\n",
    "            'secretKey': 'ljPLgCtzOKLZuG56DPX9hvrVFHRsWHqf'},\n",
    "           {'appId': '16306475', # 杨老板\n",
    "            'apiKey': 'yehP04KXoWADTGUyLF8WK0hN',\n",
    "            'secretKey': 'jfp3wxqupbWfEoNi7L316LdfbUmDmRUo'},\n",
    "           ]\n",
    "tokens = list(get_token(accounts))\n",
    "baidu_clients = [AipNlp(**account) for account in accounts] \n",
    "\n",
    "# MongoDB-pymongo\n",
    "pymongo_client = MongoClient('localhost', 27018)\n",
    "db = pymongo_client.exchange\n",
    "coll_emb_bd = db.emb_bd\n",
    "coll_vocab_without_emb = db.vocab_without_emb\n",
    "coll_vocab_to_request = db.vocab_to_request\n",
    "coll_vocab_pos = db.vocab_pos\n",
    "coll_vocab_ne = db.vocab_ne\n",
    "coll_text_seg_bd = db.text_seg_bd\n",
    "coll_text_seg_pku = db.text_seg_pku\n",
    "coll_title_seg_bd = db.title_seg_bd\n",
    "coll_title_seg_pku = db.title_seg_pku\n",
    "coll_test = db.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in vocab_dup:\n",
    "    with capture_output():\n",
    "        coll_vocab_to_request.delete_one({'word':word})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_with_emb: 396171\n",
      "vocab_without_emb: 7636551\n",
      "vocab_request: 898321\n",
      "Wall time: 35.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# vocab_last\n",
    "vocab_with_emb = set(doc['word'] for doc in coll_emb_bd.find(projection={'word':1, '_id':0}))\n",
    "print(f'vocab_with_emb: {len(vocab_with_emb)}')\n",
    "   \n",
    "# vocab_without_emb\n",
    "vocab_without_emb = set(doc['word'] for doc in coll_vocab_without_emb.find(projection={'word':1, '_id':0}))\n",
    "print(f'vocab_without_emb: {len(vocab_without_emb)}')\n",
    "\n",
    "# delete dup in coll_vocab_to_request\n",
    "vocab_to_request = set(doc['word'] for doc in coll_vocab_to_request.find(projection={'word':1, '_id':0}))\n",
    "vocab_dup =  (vocab_to_request & vocab_with_emb) | (vocab_to_request & vocab_without_emb)\n",
    "\n",
    "for word in vocab_dup:\n",
    "    with capture_output():\n",
    "        coll_vocab_to_request.delete_one({'word':word})\n",
    "\n",
    "# vocab_request\n",
    "vocab_request = vocab_to_request - vocab_with_emb - vocab_without_emb\n",
    "\n",
    "# # vocab from GBK\n",
    "# gbk_path = os.path.expanduser('~')+'/OneDrive/Data/gbk-list.txt'\n",
    "# gbk_list = []\n",
    "# with open(gbk_path, encoding='utf-8') as f:\n",
    "#     for line in f:\n",
    "#         line = line.replace('\\n', '').strip()\n",
    "#         gbk_list.append(line)\n",
    "# vocab_request = set(gbk_list) - vocab_without_emb - vocab_last\n",
    "\n",
    "print(f'vocab_request: {len(vocab_request)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start creating tasks... 08:08:44\n",
      "tasks succesfully created 08:08:58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                                      | 5750/898321 [02:59<6:04:30, 40.81it/s]"
     ]
    }
   ],
   "source": [
    "async def get_emb(word, token):\n",
    "    base_url = 'https://aip.baidubce.com/rpc/2.0/nlp/v2/word_emb_vec'\n",
    "    url_params = {'access_token': token,\n",
    "                  'charset': 'UTF-8'}\n",
    "    request_text = {'word': word}\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.request('POST', url=base_url, json=request_text, headers=headers, params=url_params) as resp:\n",
    "            response = await resp.json()\n",
    "            return response\n",
    "\n",
    "async def request_one(word, token, bucket, download_delay):\n",
    "    global count_all, last_count_all, count_success,\\\n",
    "        count_non_exist, count_input_too_long, count_input_empty,\\\n",
    "        count_limit_reached, count_unknow_error,\\\n",
    "        count_mongo_error, count_duplicate, t0\n",
    "    try:\n",
    "        await asyncio.sleep(download_delay)\n",
    "        async with bucket:\n",
    "                response = await get_emb(word, token)\n",
    "        if 'error_code' in response:\n",
    "            if response['error_code'] == 18: # limit reached\n",
    "                count_limit_reached += 1\n",
    "            elif response['error_code'] == 282131: # input too long\n",
    "                count_input_too_long += 1\n",
    "            elif response['error_code'] == 282134: # input empty\n",
    "                count_input_empty += 1\n",
    "            elif response['error_code'] == 282300: # non-existing\n",
    "                count_non_exist += 1\n",
    "                with capture_output() as captured:\n",
    "                    coll_vocab_without_emb.insert_one({'word': word})\n",
    "            else:\n",
    "                print(f'Other error_code: word:\"{word}\", response:\"{response}\"')\n",
    "                count_unknow_error += 1\n",
    "        elif 'vec' in response:\n",
    "            vec = response['vec']\n",
    "            with capture_output() as captured:\n",
    "                assert type(vec) is list, 'type(vec) is not list: {word}'\n",
    "                assert len(vec) == 1024, 'len(vec) should be 1024'\n",
    "                coll_emb_bd.insert_one({'word': word, 'vec': vec})\n",
    "                count_success += 1\n",
    "    except DuplicateKeyError:\n",
    "        count_duplicate += 1\n",
    "    except:\n",
    "        print(f\"Unknow exception: {sys.exc_info()[:-1]}, {Now()}\")\n",
    "        count_unknow_error += 1\n",
    "    finally:\n",
    "        if count_all % 10000 == 0:\n",
    "            te = time.time()\n",
    "            print(f'{count_all}/{len(vocab_request)}, {Now()} ({te-t0:.1f}s / {(count_all-last_count_all)/(te-t0):.1f}QPS)')\n",
    "            print(f'success:{count_success} non_existing:{count_non_exist} duplicate:{count_duplicate} limit_reached:{count_limit_reached} input_too_long:{count_input_too_long} input_empty:{count_input_empty} unkown_error:{count_unknow_error}')\n",
    "            print('---------------------')\n",
    "            t0 = time.time()\n",
    "            count_success, count_non_exist, count_duplicate, count_input_too_long, count_input_empty,\\\n",
    "            count_limit_reached, count_unknow_error, count_mongo_error = 0, 0, 0, 0, 0, 0, 0, 0\n",
    "            last_count_all = count_all\n",
    "    \n",
    "async def download_many(tokens, vocab_request, max_concur_req, download_delay):\n",
    "    global count_all\n",
    "    tokens_iter = itertools.cycle(tokens)\n",
    "    # restrict concurrenty\n",
    "    bucket = AsyncLeakyBucket(max_concur_req, 1)\n",
    "    # generate tasks\n",
    "    print(f'start creating tasks... {Now()}')\n",
    "    tasks = []\n",
    "    for word in vocab_request:\n",
    "        task = request_one(word, next(tokens_iter), bucket, download_delay)\n",
    "        tasks.append(task)\n",
    "    print(f'tasks succesfully created {Now()}')\n",
    "        \n",
    "    # schedule tasks\n",
    "    for future in tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n",
    "        count_all += 1\n",
    "        await future\n",
    "\n",
    "# run the main\n",
    "# vocab_request = [doc['word'] for doc in coll_vocab_to_request.find(projection={'word':1, '_id':0})][:300]\n",
    "# coll_test.delete_many({})\n",
    "t0 = time.time()\n",
    "count_all, last_count_all, count_success, count_non_exist, count_input_too_long, count_input_empty, count_limit_reached, count_unknow_error, count_mongo_error, count_duplicate = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "asyncio.run(download_many(tokens, vocab_request, max_concur_req=32, download_delay=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

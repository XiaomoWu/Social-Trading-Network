{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rossz\\OneDrive\\STN\\10-FirstYear\\code\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import aiohttp\n",
    "import pkuseg\n",
    "import pymongo\n",
    "import zhconv\n",
    "from aip import AipNlp\n",
    "from motor import motor_asyncio\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import DuplicateKeyError\n",
    "    \n",
    "# CONFIG\n",
    "if platform.system() == 'Windows':\n",
    "    # CODE_PATH\n",
    "    CODE_PATH = os.path.join(os.environ['onedrive'], 'STN/10-FirstYear/code')\n",
    "    DATA_PATH = os.path.join(os.environ['onedrive'], 'STN/10-FirstYear/data')\n",
    "    %cd {CODE_PATH}\n",
    "elif platform.system() == 'Linux':\n",
    "    # CODE_PATH\n",
    "    CODE_PATH = '/mnt/c/Users/rossz/OneDrive/STN/10-FirstYear/code'\n",
    "    DATA_PATH = '/mnt/c/Users/rossz/OneDrive/STN/10-FirstYear/data'\n",
    "    %cd {CODE_PATH}\n",
    "\n",
    "# Baidu NLP\n",
    "def get_token(accounts):\n",
    "    '''get API tokens'''\n",
    "    url = f\"https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials\"\n",
    "    for account in accounts:\n",
    "        # client_id 为官网获取的AK， client_secret 为官网获取的SK\n",
    "        param = {'client_id': account['apiKey'],\n",
    "                 'client_secret': account['secretKey']}\n",
    "        resp = requests.get(url, param).json()\n",
    "        yield resp['access_token']\n",
    "        \n",
    "accounts = [{'appId': '16105374', # ross.zhu@outlook.com\n",
    "            'apiKey': 'zShobECEYX2B2CNrcz3QsM3I',\n",
    "            'secretKey': 'xM8g1lkhTd8b7DYGv2H9j7g2hvthPGE5'},\n",
    "           {'appId': '16218655', # 18868819096\n",
    "            'apiKey': 'AG2ygGZbCPqDW25ZC6FinQRy',\n",
    "            'secretKey': 'pcx8pFwjVhD0oGDsdm58bzt7Zm9zFpWk'},\n",
    "           {'appId': '16226200', # 13566633705\n",
    "            'apiKey': 'ymmGiPQoAoSF2KhcGVKNZEVj',\n",
    "            'secretKey': '78oQXsRtdmRcINEGtP3aP2y4sKA7HerU'},\n",
    "           {'appId': '16226243', # 13606586786\n",
    "            'apiKey': 'jIy3T25rYR1Kj9BlCKPcHbYe',\n",
    "            'secretKey': 'jORNEeu7Qnl6L1Hr07NomigmsUtXlloL'},\n",
    "           {'appId': '16230746', # 李锐\n",
    "            'apiKey': 't6sWsoIYinoGBgwbLiCopQHR',\n",
    "            'secretKey': 'evq4UygxQ555LXzVzaDWQS2TfGNUHrYf'},\n",
    "           {'appId': '16307961', # 俞嘉炜\n",
    "            'apiKey': 'TKIEhGe1fIY89VlYm8yjuDZW',\n",
    "            'secretKey': '6kDBxZnGZEsK4y9ntg94VOBSIjLS0UQs'},\n",
    "           {'appId': '16305932', # 芳菲\n",
    "            'apiKey': 'Y7NjBe2sqhfIjSIaB51wvlHH',\n",
    "            'secretKey': 'lqLW2XeSj0xBvCbUASh0E2wuMmW6e4ox'},\n",
    "           {'appId': '16305931', # 小毫生\n",
    "            'apiKey': 'D2GGEWS80KsU9mtft1WnuR0N',\n",
    "            'secretKey': '8FbvLeGD5aXLL6bpKSEZoG7CRt1Q9oqo'},\n",
    "           {'appId': '16306399', # 忻小宇\n",
    "            'apiKey': 'E89qU1OMrwvyanH7xixkFVrR',\n",
    "            'secretKey': 'ljPLgCtzOKLZuG56DPX9hvrVFHRsWHqf'},\n",
    "           {'appId': '16306475', # 杨老板\n",
    "            'apiKey': 'yehP04KXoWADTGUyLF8WK0hN',\n",
    "            'secretKey': 'jfp3wxqupbWfEoNi7L316LdfbUmDmRUo'},\n",
    "           ]\n",
    "tokens = list(get_token(accounts))\n",
    "baidu_clients = [AipNlp(**account) for account in accounts] \n",
    "\n",
    "# MongoDB-pymongo\n",
    "pymongo_client = MongoClient('localhost', 27018)\n",
    "db = pymongo_client.exchange\n",
    "coll_embedding_bd = db.embedding_bd\n",
    "coll_vocab_without_emb = db.vocab_without_emb\n",
    "coll_vocab_to_request = db.vocab_to_request\n",
    "coll_vocab_pos = db.vocab_pos\n",
    "coll_vocab_ne = db.vocab_ne\n",
    "coll_text_seg_bd = db.text_seg_bd\n",
    "coll_title_seg_bd = db.title_seg_bd\n",
    "coll_test = db.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6894896 docs have been segged.\n",
      "37510263 docs to reqeust.\n",
      "6748490 words to request\n",
      "Wall time: 2min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def build_requests(cmt_path, cmt_coll):\n",
    "    cmt_text = pickle.load(open(cmt_path, 'rb'))\n",
    "    cmt_text = [(i, x) for i, x in enumerate(cmt_text)]\n",
    "\n",
    "    segged_cmt_id = set(x['id'] for x in cmt_coll.find(projection={'id':1, '_id':0}))\n",
    "    print(f'{len(segged_cmt_id)} docs have been segged.')\n",
    "    \n",
    "    request_cmt_id = set(range(len(cmt_text))) - segged_cmt_id\n",
    "    request_cmt = [(i, doc) for i, doc in cmt_text if i in request_cmt_id]\n",
    "    print(f'{len(request_cmt_id)} docs to reqeust.')\n",
    "    \n",
    "    vocab_to_request = set(x['word'] for x in coll_vocab_to_request.find(projection={'word':1, '_id':0}))\n",
    "    print(f'{len(vocab_to_request)} words to request')\n",
    "    \n",
    "    return request_cmt, vocab_to_request\n",
    "\n",
    "# request_cmt_title, request_cmt_id = build_requests('../data/cmt_title_clean.pkl', coll_title_seg_bd)\n",
    "request_cmt_text, vocab_to_request = build_requests('../data/cmt_text_clean.pkl', coll_text_seg_bd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                   | 20908/5000000 [13:29<44:35:55, 31.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknow Exception: (<class 'aiohttp.client_exceptions.ContentTypeError'>, ContentTypeError(\"0, message='Attempt to decode JSON with unexpected mimetype: text/html;charset=utf-8'\"))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                                   | 28505/5000000 [17:30<43:33:59, 31.70it/s]"
     ]
    }
   ],
   "source": [
    "async def get_lex(text, token):\n",
    "    base_url = 'https://aip.baidubce.com/rpc/2.0/nlp/v1/lexer'\n",
    "    url_params = {'access_token': token,\n",
    "                  'charset': 'UTF-8'}\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    request_text = {'text': text}\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.request('POST', url=base_url, json=request_text, headers=headers, params=url_params) as resp:\n",
    "            response = await resp.json()\n",
    "            return response\n",
    "        \n",
    "async def seg_one(idx, text, token, coll_cmt, bucket, download_delay, len_tasks):\n",
    "    global t0, count_all, last_count_all, count_success, count_empty_input,\\\n",
    "        count_input_too_long, count_limit_reached,\\\n",
    "        count_other_error, count_duplicate, count_mongo_error,\\\n",
    "        vocab_to_request\n",
    "    try:\n",
    "        \n",
    "        # if empty text, directly write to db\n",
    "        # else, send request and get response\n",
    "        if text == '':\n",
    "            count_empty_input += 1\n",
    "            coll_cmt.insert_one({'id': idx, 'text_seg': '', 'text_seg2': ''})\n",
    "        else:\n",
    "            await asyncio.sleep(download_delay)\n",
    "            # if longer than 20000 bytes, truncate\n",
    "            is_too_long = False\n",
    "            if len(text.encode('utf-8')) >= 20000:\n",
    "                text = text.encode('utf-8')[:20000].decode('utf-8', errors='ignore')\n",
    "                is_too_long = True\n",
    "            async with bucket:\n",
    "                    response = await get_lex(text, token)\n",
    "\n",
    "        # process errors in response\n",
    "        # if no error, generate input doc\n",
    "        if 'error_code' in response:\n",
    "            if response['error_code'] == 18: # limit reached\n",
    "                count_limit_reached += 1\n",
    "            elif response['error_code'] == 282131: # input too long\n",
    "                count_input_too_long += 1\n",
    "            else:\n",
    "                print(f'other baidu error: request:\"{text[:20]}...\", response:\"{response}\"')\n",
    "                print(token)\n",
    "                count_other_error += 1\n",
    "        else:\n",
    "            # process successful response\n",
    "            response = response['items']\n",
    "            vocab_coarse = [item['item'] for item in response]\n",
    "            vocab_granular = list(itertools.chain.from_iterable([item['basic_words'] for item in response]))\n",
    "            text_seg = ' '.join(vocab_coarse)\n",
    "            text_seg2 = ' '.join(vocab_granular)\n",
    "            pos = {item['item']: item['pos'] for item in response if item['pos'] != ''}\n",
    "            ne = {item['item']: item['ne'] for item in response if item['ne'] != ''}\n",
    "\n",
    "            # write coll_text_seg_bd; coll_vocab_to_request\n",
    "            with capture_output():\n",
    "                coll_cmt.insert_one({'id': idx, 'text_seg': text_seg, 'text_seg2': text_seg2, 'is_too_long': is_too_long})\n",
    "                count_success += 1\n",
    "                for word in itertools.chain(vocab_coarse, vocab_granular):\n",
    "                    if word not in vocab_to_request:\n",
    "                        coll_vocab_to_request.insert_one({'word': word})\n",
    "                \n",
    "    except DuplicateKeyError:\n",
    "        count_duplicate += 1\n",
    "    except:\n",
    "        print(f'Unknow Exception: {sys.exc_info()[:-1]}')\n",
    "        count_other_error += 1\n",
    "    finally:\n",
    "    # log\n",
    "        if count_all % 10000 == 0:\n",
    "            print(f'{count_all}/{len_tasks}, {Now()} ({time.time()-t0:.1f}s / {(count_all-last_count_all)/(time.time()-t0):.1f}QPS)')\n",
    "            print(f'success:{count_success}, limit-reached:{count_limit_reached}, empty-input-in-success:{count_empty_input}, dup:{count_duplicate}, input-too-long:{count_input_too_long}, other-baidu-error:{count_other_error}, other-mongo-error:{count_mongo_error}')\n",
    "            print('---------------------')\n",
    "            count_success, count_empty_input, count_input_too_long, count_limit_reached, count_other_error, count_duplicate, count_mongo_error = 0, 0, 0, 0, 0, 0, 0\n",
    "            t0 = time.time()\n",
    "            last_count_all = count_all\n",
    "    \n",
    "    \n",
    "async def download_many(tokens, request_cmt, coll_cmt, max_concur_req, download_delay):\n",
    "    global count_all\n",
    "    tokens_iter = itertools.cycle(tokens)\n",
    "    len_tasks = len(request_cmt)\n",
    "    # restrict concurrenty\n",
    "    bucket = AsyncLeakyBucket(max_concur_req, 1)\n",
    "    \n",
    "    # generate tasks\n",
    "    print(f'generating tasks...{Now()}')\n",
    "    tasks = []\n",
    "    for idx, text in request_cmt:\n",
    "        task = seg_one(idx, text, next(tokens_iter), coll_cmt, bucket, download_delay, len_tasks)\n",
    "        tasks.append(task)\n",
    "    print(f'tasks generated {Now()}')\n",
    "        \n",
    "    # schedule tasks\n",
    "    for future in tqdm(asyncio.as_completed(tasks), total=len_tasks):\n",
    "        count_all += 1\n",
    "        await future\n",
    "\n",
    "# run the main\n",
    "t0 = time.time()\n",
    "count_all, last_count_all, count_success, count_empty_input, count_input_too_long, count_limit_reached, count_other_error, count_duplicate, count_mongo_error = 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "\n",
    "asyncio.run(download_many(tokens, request_cmt_text[:5000000], coll_text_seg_bd, max_concur_req=31, download_delay=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
